


# Install requirements
!pip install -r requirements.txt | grep -v "already satisfied"





from src.helpers import setup_env

# If running locally, this will download dataset (make sure you have at 
# least 2 Gb of space on your hard drive)
setup_env()





!pytest -vv src/data.py -k data_loaders














!pytest -vv src/data.py -k visualize_one_batch





%matplotlib inline
from src.data import visualize_one_batch, get_data_loaders

# use get_data_loaders to get the data_loaders dictionary. Use a batch_size
# of 5, a validation size of 0.01 and num_workers=-1 (all CPUs)
batch_size = 5
valid_size = 0.01
num_workers = 1
data_loaders = get_data_loaders(batch_size=batch_size, valid_size=valid_size, num_workers=num_workers)

visualize_one_batch(data_loaders)





!pytest -vv src/model.py











!pytest -vv src/optimization.py -k get_loss





!pytest -vv src/optimization.py -k get_optimizer





!pytest -vv src/train.py -k train_one_epoch





!pytest -vv src/train.py -k valid_one_epoch





!pytest -vv src/train.py -k optimize





!pytest -vv src/train.py -k one_epoch_test





batch_size = 32      # size of the minibatch for stochastic gradient descent (or Adam)
valid_size = 0.2       # fraction of the training data to reserve for validation
num_epochs = 70        # number of epochs for training
num_classes = 50       # number of classes. Do not change this
dropout = 0.5          # dropout for our model
learning_rate = 0.001  # Learning rate for SGD (or Adam)
opt = 'adam'            # optimizer. 'sgd' or 'adam'
weight_decay = 0.001   # regularization. Increase this to combat overfitting


from src.data import get_data_loaders
from src.train import optimize
from src.optimization import get_optimizer, get_loss
from src.model import MyModel
import torch

# get the data loaders using batch_size and valid_size defined in the previous cell
data_loaders = get_data_loaders(
    batch_size=batch_size, 
    valid_size=valid_size, 
    num_workers=1
)

# instance model MyModel with num_classes and dropout defined in the previous cell
model = MyModel(num_classes=num_classes, dropout=dropout)
checkpoint = torch.load('checkpoints/best_val_loss.pt')
model.load_state_dict(checkpoint)
# Get the optimizer using get_optimizer and the model you just created, the learning rate, 
# the optimizer and the weight decay specified in the previous cell
optimizer = get_optimizer(
    model, 
    optimizer=opt, 
    learning_rate=learning_rate, 
    momentum=0.9, 
    weight_decay=weight_decay
)

# Get the loss using get_loss
loss = get_loss()

# Train and optimize the model
optimize(
    data_loaders,
    model,
    optimizer,
    loss,
    n_epochs=num_epochs,
    save_path="checkpoints/best_val_loss.pt",
    interactive_tracking=True
)






# load the model that got the best validation accuracy
from src.train import one_epoch_test
from src.model import MyModel
import torch

model = MyModel(num_classes=num_classes, dropout=dropout)

# YOUR CODE HERE: load the weights in 'checkpoints/best_val_loss.pt'
checkpoint = torch.load('checkpoints/best_val_loss.pt')
model.load_state_dict(checkpoint)

# Run test
one_epoch_test(data_loaders['test'], model, loss)





!pytest -vv src/predictor.py





# NOTE: you might need to restart the notebook before running this step
# If you get an error about RuntimeError: Can't redefine method: forward on class
# restart your notebook then execute only this cell
from src.predictor import Predictor
from src.helpers import compute_mean_and_std
from src.model import MyModel
from src.data import get_data_loaders
import torch

data_loaders = get_data_loaders(batch_size=1)

# First let's get the class names from our data loaders
class_names = data_loaders["train"].dataset.classes

# Then let's move the model_transfer to the CPU
# (we don't need GPU for inference)
model = MyModel(num_classes=50, dropout=0.5).cpu()

# Let's make sure we use the right weights by loading the
# best weights we have found during training
# NOTE: remember to use map_location='cpu' so the weights
# are loaded on the CPU (and not the GPU)

# YOUR CODE HERE

# Let's wrap our model using the predictor class
mean, std = compute_mean_and_std()
predictor = Predictor(model, class_names, mean, std).cpu()

# Export using torch.jit.script
scripted_predictor = # YOUR CODE HERE

scripted_predictor.save("checkpoints/original_exported.pt")





import torch

# Load using torch.jit.load
model_reloaded =  # YOUR CODE HERE


from src.predictor import predictor_test

pred, truth = predictor_test(data_loaders['test'], model_reloaded)





from src.helpers import plot_confusion_matrix

plot_confusion_matrix(pred, truth)



